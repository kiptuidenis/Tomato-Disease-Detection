{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiptuidenis/Tomato-Disease-Detection/blob/main/tomato_disease_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb5cc086",
      "metadata": {
        "id": "eb5cc086"
      },
      "source": [
        "# Tomato Disease Classification using CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8273507",
      "metadata": {
        "id": "e8273507"
      },
      "source": [
        "This notebook builds a Convolutional Neural Network (CNN) to classify images of tomato leaves into different disease categories. It also includes the capability to identify images that are not tomato leaves."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af25a669",
      "metadata": {
        "id": "af25a669"
      },
      "source": [
        "## 1. Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "57d9331b",
      "metadata": {
        "id": "57d9331b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "6d33cb49-c407-4d63-b99e-b1e56d4c55c4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Config option jax_dump_ir_to already defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2310748360.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m \u001b[0;31m# line: 125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m \u001b[0;31m# line: 125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpsSet\u001b[0m \u001b[0;31m# line: 153\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtoco_convert\u001b[0m \u001b[0;31m# line: 937\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauthoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelAnalyzer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAnalyzer\u001b[0m \u001b[0;31m# line: 35\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpResolverType\u001b[0m \u001b[0;31m# line: 315\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatible\u001b[0m \u001b[0;31m# line: 263\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_wrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/authoring/authoring.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverter_error_data_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_tf_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstablehlo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantization_options_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mquant_opts_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite_constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_phase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_phase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0m_jit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypeof\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtypeof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meffects_barrier\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0meffects_barrier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mblock_until_ready\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mblock_until_ready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mad_checkpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheckpoint_wrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcheckpoint\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m from jax._src.tree_util import (\n\u001b[1;32m     42\u001b[0m     \u001b[0mtree_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_transpose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/stages.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msharding_impls\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnspecifiedValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAUTO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmlir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlir\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxla_client\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/mlir.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mMYPY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m _JAX_DUMP_IR_TO = config.string_flag(\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;34m'jax_dump_ir_to'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'JAX_DUMP_IR_TO'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Path to which the IR that is emitted by JAX should be dumped as \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/config.py\u001b[0m in \u001b[0;36mstring_flag\u001b[0;34m(name, default, *args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m   \u001b[0mupdate_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update_hook\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m   \u001b[0mholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_hook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m   \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/config.py\u001b[0m in \u001b[0;36madd_option\u001b[0;34m(self, name, holder, opt_type, meta_args, meta_kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0madd_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_holders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Config option {name} already defined\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_holders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mopt_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Config option jax_dump_ir_to already defined"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil # For creating a 'non_tomato' class if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "st8uiuVwkL4t",
      "metadata": {
        "id": "st8uiuVwkL4t"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e570eb40",
      "metadata": {
        "id": "e570eb40"
      },
      "source": [
        "## 2. Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69fbc51f",
      "metadata": {
        "id": "69fbc51f"
      },
      "outputs": [],
      "source": [
        "# Define paths and parameters\n",
        "base_dir = '/content/drive/MyDrive/Tomatoes'\n",
        "image_height = 128\n",
        "image_width = 128\n",
        "batch_size = 32\n",
        "\n",
        "# Image Data Generators with Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2 # Splitting 20% of the data for validation\n",
        ")\n",
        "\n",
        "# For the validation/test set, we only rescale\n",
        "# Note: It's better to have a separate test_datagen if test set augmentation differs or is not desired.\n",
        "# For simplicity here, validation_generator will use the same augmentation settings as train_datagen (excluding validation_split part)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2) # Also applying validation_split here to ensure it picks up the correct subset\n",
        "\n",
        "# Flow training images in batches using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(image_height, image_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training', # Set as training data\n",
        "    # Add a seed for reproducibility if desired\n",
        "    # seed=42\n",
        ")\n",
        "\n",
        "# Flow validation images in batches using the same train_datagen instance but with subset='validation'\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(image_height, image_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation', # Set as validation data\n",
        "    # seed=42 # Use the same seed if you want consistent split\n",
        ")\n",
        "\n",
        "\n",
        "# Get class names and number of classes\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "num_classes = len(class_names)\n",
        "print(f\"Found {train_generator.samples} images belonging to {num_classes} classes for training.\")\n",
        "print(f\"Found {validation_generator.samples} images belonging to {num_classes} classes for validation.\")\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "# Handling 'not tomato leaves'\n",
        "# Create a 'non_tomato' directory if it doesn't exist.\n",
        "# You MUST populate this directory with images that are NOT tomato leaves for the model to learn this category.\n",
        "non_tomato_dir = os.path.join(base_dir, 'non_tomato')\n",
        "if not os.path.exists(non_tomato_dir):\n",
        "    os.makedirs(non_tomato_dir)\n",
        "    print(f\"Created directory: {non_tomato_dir}. Please add non-tomato images to this folder and re-run data loading.\")\n",
        "    # IMPORTANT: After adding images to 'non_tomato', you need to re-run the cell above that defines\n",
        "    # train_generator and validation_generator so they can pick up the new class.\n",
        "    # If 'non_tomato' remains empty, the model won't learn to classify them.\n",
        "\n",
        "# After potentially adding the 'non_tomato' class and its images,\n",
        "# it's crucial to re-initialize the ImageDataGenerators to include this new class.\n",
        "# For this initial run, if 'non_tomato' was just created and is empty,\n",
        "# the class_names and num_classes will reflect only the original folders.\n",
        "# The user needs to add images and re-run the ImageDataGenerator cells."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f06c19b",
      "metadata": {
        "id": "9f06c19b"
      },
      "source": [
        "## 3. Visualize the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84ccd88e",
      "metadata": {
        "id": "84ccd88e"
      },
      "outputs": [],
      "source": [
        "def plot_sample_images(generator, class_names_list):\n",
        "    images, labels = next(generator) # Get a batch of images and labels\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    for i in range(min(9, len(images))): # Display up to 9 images\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        # Ensure class_names_list is correctly indexed\n",
        "        try:\n",
        "            plt.title(class_names_list[np.argmax(labels[i])])\n",
        "        except IndexError:\n",
        "            plt.title(\"Unknown Class\") # Fallback if index is out of bounds\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Ensure class_names is up-to-date after potential 'non_tomato' addition and re-run of generators\n",
        "current_class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "print(\"Sample Training Images:\")\n",
        "plot_sample_images(train_generator, current_class_names)\n",
        "\n",
        "print(\"\\nSample Validation Images:\")\n",
        "plot_sample_images(validation_generator, current_class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e078477",
      "metadata": {
        "id": "3e078477"
      },
      "source": [
        "## 4. Build the CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87765803",
      "metadata": {
        "id": "87765803"
      },
      "outputs": [],
      "source": [
        "# Ensure num_classes is updated if 'non_tomato' class was added and generators were re-run\n",
        "current_num_classes = len(train_generator.class_indices)\n",
        "\n",
        "model = Sequential([\n",
        "    # First convolutional block\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Second convolutional block\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Third convolutional block\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Fourth convolutional block\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.5), # Dropout for regularization\n",
        "    layers.Dense(current_num_classes, activation='softmax') # Output layer: num_classes neurons for multi-class classification\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f0a02f",
      "metadata": {
        "id": "42f0a02f"
      },
      "source": [
        "## 5. Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01c0b13e",
      "metadata": {
        "id": "01c0b13e"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "442d8f63",
      "metadata": {
        "id": "442d8f63"
      },
      "source": [
        "## 6. Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dcfe380",
      "metadata": {
        "id": "8dcfe380"
      },
      "outputs": [],
      "source": [
        "# Define callbacks\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, min_lr=0.00001)\n",
        "\n",
        "epochs = 10 # Adjust as needed\n",
        "\n",
        "# Check if generators are empty (can happen if paths are wrong or no images found)\n",
        "if train_generator.samples == 0:\n",
        "    print(\"Error: Training generator is empty. Check dataset paths and image files.\")\n",
        "elif validation_generator.samples == 0:\n",
        "    print(\"Error: Validation generator is empty. Check dataset paths and image files or validation_split.\")\n",
        "else:\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=max(1, train_generator.samples // batch_size), # Ensure steps_per_epoch is at least 1\n",
        "        epochs=epochs,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=max(1, validation_generator.samples // batch_size), # Ensure validation_steps is at least 1\n",
        "        callbacks=[early_stopping, reduce_lr]\n",
        "    )\n",
        "    print(\"Model training finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e02542",
      "metadata": {
        "id": "35e02542"
      },
      "source": [
        "## 7. Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a380fce7",
      "metadata": {
        "id": "a380fce7"
      },
      "outputs": [],
      "source": [
        "if 'history' in locals() and history is not None:\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluate on the validation set (acting as a test set for now)\n",
        "    print(\"\\nEvaluating model on validation data...\")\n",
        "    if validation_generator.samples > 0:\n",
        "        val_loss_eval, val_acc_eval = model.evaluate(validation_generator, steps=max(1, validation_generator.samples // batch_size))\n",
        "        print(f\"Validation Loss: {val_loss_eval}\")\n",
        "        print(f\"Validation Accuracy: {val_acc_eval}\")\n",
        "    else:\n",
        "        print(\"Validation generator is empty, cannot evaluate.\")\n",
        "else:\n",
        "    print(\"Model was not trained (history object not found). Skipping evaluation plots.\")\n",
        "\n",
        "# Note: For a proper final evaluation, you should use a separate, unseen test set.\n",
        "# Create a test_generator similar to train_generator and validation_generator, pointing to a 'test' directory.\n",
        "# test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
        "# print(f\"Test Loss: {test_loss}\")\n",
        "# print(f\"Test Accuracy: {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b661760",
      "metadata": {
        "id": "5b661760"
      },
      "source": [
        "## 8. Test with New Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ebdfae9",
      "metadata": {
        "id": "4ebdfae9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "def predict_image(img_path, model_to_test, class_names_list, img_height, img_width, non_tomato_confidence_threshold=0.5):\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"Error: Image path not found: {img_path}\")\n",
        "        return None, None\n",
        "\n",
        "    img = image.load_img(img_path, target_size=(img_height, img_width))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0) # Create a batch\n",
        "    img_array /= 255. # Rescale\n",
        "\n",
        "    predictions = model_to_test.predict(img_array)\n",
        "    predicted_class_index = np.argmax(predictions[0])\n",
        "    confidence = np.max(predictions[0])\n",
        "    predicted_class_name = class_names_list[predicted_class_index]\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Logic for 'not tomato leaf' detection\n",
        "    # This is more effective if 'non_tomato' is a trained class.\n",
        "    if 'non_tomato' in class_names_list and predicted_class_name == 'non_tomato':\n",
        "        title_text = f\"Predicted: Not a Tomato Leaf (as 'non_tomato' class)\\nConfidence: {confidence*100:.2f}%\"\n",
        "    elif confidence < non_tomato_confidence_threshold and 'non_tomato' not in class_names_list:\n",
        "        # If 'non_tomato' is not a class, use threshold for other classes\n",
        "        title_text = f\"Predicted: Unsure / Likely Not Tomato Leaf\\n(Low confidence: {confidence*100:.2f}% for {predicted_class_name})\"\n",
        "        predicted_class_name = \"Unknown/Non-Tomato (Low Confidence)\"\n",
        "    elif confidence < non_tomato_confidence_threshold and predicted_class_name != 'non_tomato' and 'non_tomato' in class_names_list:\n",
        "        # If 'non_tomato' IS a class, but another class has low confidence\n",
        "        title_text = f\"Predicted: {predicted_class_name} (Low Confidence: {confidence*100:.2f}%)\\nConsider if it might be 'non_tomato'.\"\n",
        "    else:\n",
        "        title_text = f\"Predicted: {predicted_class_name}\\nConfidence: {confidence*100:.2f}%\"\n",
        "\n",
        "    plt.title(title_text)\n",
        "    plt.show()\n",
        "\n",
        "    print(title_text.replace('\\n', ' '))\n",
        "    return predicted_class_name, confidence\n",
        "\n",
        "# --- Example Usage (Replace with your actual image paths!) ---\n",
        "# Ensure class_names is up-to-date\n",
        "current_class_names_for_prediction = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Path to an image from your 'healthy' class\n",
        "# IMPORTANT: Replace this with an actual path to one of your healthy tomato leaf images.\n",
        "dummy_healthy_img_path = os.path.join(base_dir, 'healthy', '0a0d5a74-bb60-418a-94fd-8f7d80101589___GH_HL Leaf 193.JPG')\n",
        "\n",
        "# Path to an image that is NOT a tomato leaf\n",
        "# IMPORTANT: Replace this with an actual path to an image that is not a tomato leaf.\n",
        "# For example, an image of a car, a cat, a different plant, or place it in the 'non_tomato' folder and test.\n",
        "dummy_non_tomato_img_path = os.path.join(base_dir, 'non_tomato', 'test_non_tomato_image.jpg') # Create this file or use another\n",
        "\n",
        "# Create a dummy non-tomato image if it doesn't exist for testing purposes\n",
        "if not os.path.exists(dummy_non_tomato_img_path) and not os.path.exists(os.path.join(base_dir, 'non_tomato')):\n",
        "    # Simple placeholder if no non_tomato image is available\n",
        "    try:\n",
        "        # Create a simple black image as a placeholder if Pillow is installed\n",
        "        from PIL import Image\n",
        "        img_placeholder = Image.new('RGB', (image_width, image_height), color = 'black')\n",
        "        # Ensure the 'non_tomato' directory exists before saving\n",
        "        if not os.path.exists(os.path.join(base_dir, 'non_tomato')):\n",
        "            os.makedirs(os.path.join(base_dir, 'non_tomato'))\n",
        "        dummy_non_tomato_img_path = os.path.join(base_dir, 'non_tomato', 'placeholder_non_tomato.png')\n",
        "        img_placeholder.save(dummy_non_tomato_img_path)\n",
        "        print(f\"Created a placeholder non-tomato image: {dummy_non_tomato_img_path}\")\n",
        "    except ImportError:\n",
        "        print(\"Pillow library not found, cannot create placeholder non-tomato image. Please provide one manually.\")\n",
        "        dummy_non_tomato_img_path = None # Set to None if cannot create\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating placeholder image: {e}\")\n",
        "        dummy_non_tomato_img_path = None\n",
        "\n",
        "if 'model' in locals(): # Check if model is trained/defined\n",
        "    print(\"\\n--- Testing with a Healthy Tomato Image ---\")\n",
        "    if os.path.exists(dummy_healthy_img_path):\n",
        "        predict_image(dummy_healthy_img_path, model, current_class_names_for_prediction, image_height, image_width, non_tomato_confidence_threshold=0.6)\n",
        "    else:\n",
        "        print(f\"Test image not found: {dummy_healthy_img_path}. Please provide a valid path.\")\n",
        "\n",
        "    print(\"\\n--- Testing with a Non-Tomato Image ---\")\n",
        "    if dummy_non_tomato_img_path and os.path.exists(dummy_non_tomato_img_path):\n",
        "        predict_image(dummy_non_tomato_img_path, model, current_class_names_for_prediction, image_height, image_width, non_tomato_confidence_threshold=0.6)\n",
        "    elif not dummy_non_tomato_img_path:\n",
        "        print(\"Skipping non-tomato image test as placeholder could not be created and no path was provided.\")\n",
        "    else:\n",
        "        print(f\"Test image not found: {dummy_non_tomato_img_path}. Please provide a valid path to a non-tomato image (or ensure the placeholder was created).\")\n",
        "\n",
        "    # Example: Test with an image from one of the disease classes\n",
        "    # dummy_disease_img_path = os.path.join(base_dir, 'Aug_bacterial_spot', 'aug_0_8655.png') # Replace with actual image\n",
        "    # print(\"\\n--- Testing with a Diseased Tomato Image ---\")\n",
        "    # if os.path.exists(dummy_disease_img_path):\n",
        "    #     predict_image(dummy_disease_img_path, model, current_class_names_for_prediction, image_height, image_width)\n",
        "    # else:\n",
        "    #     print(f\"Test image not found: {dummy_disease_img_path}. Please provide a valid path.\")\n",
        "else:\n",
        "    print(\"Model not defined or trained. Skipping tests with new images.\")\n",
        "\n",
        "print(\"\\nReminder: For best 'not tomato leaf' detection, train the model with a dedicated 'non_tomato' class containing diverse examples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60b72ddc",
      "metadata": {
        "id": "60b72ddc"
      },
      "source": [
        "## 9. Save and Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dc39b56",
      "metadata": {
        "id": "3dc39b56"
      },
      "outputs": [],
      "source": [
        "if 'model' in locals():\n",
        "    model_save_path = os.path.join(base_dir, 'tomato_disease_cnn_model.h5')\n",
        "    model.save(model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "    # Example of loading the model:\n",
        "    # loaded_model = keras.models.load_model(model_save_path)\n",
        "    # print(f\"Model loaded from {model_save_path}\")\n",
        "\n",
        "    # You can then use loaded_model to make predictions:\n",
        "    # if os.path.exists(dummy_healthy_img_path):\n",
        "    #     print(\"\\n--- Testing Loaded Model with a Healthy Tomato Image ---\")\n",
        "    #     predict_image(dummy_healthy_img_path, loaded_model, current_class_names_for_prediction, image_height, image_width)\n",
        "    # else:\n",
        "    #     print(f\"Test image not found: {dummy_healthy_img_path} for loaded model test.\")\n",
        "else:\n",
        "    print(\"Model not defined. Skipping model saving.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yTaULWAf0PPu",
      "metadata": {
        "id": "yTaULWAf0PPu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Define the path to the saved Keras model\n",
        "# This should be the same path where you saved your trained model\n",
        "base_dir = '/content/drive/MyDrive/Tomatoes'\n",
        "keras_model_path = os.path.join(base_dir, 'tomato_disease_cnn_model.h5')\n",
        "\n",
        "# Define the path for the TFLite model\n",
        "tflite_model_path = os.path.join(base_dir, 'tomato_disease_model.tflite')\n",
        "\n",
        "# Check if the Keras model file exists\n",
        "if not os.path.exists(keras_model_path):\n",
        "    print(f\"ERROR: Saved Keras model not found at {keras_model_path}\")\n",
        "    print(\"Please ensure you have trained and saved the model in the previous steps.\")\n",
        "else:\n",
        "    print(f\"Loading Keras model from: {keras_model_path}\")\n",
        "    # Load the Keras model\n",
        "    try:\n",
        "        model = tf.keras.models.load_model(keras_model_path)\n",
        "        print(\"Keras model loaded successfully.\")\n",
        "\n",
        "        # Convert the model to TensorFlow Lite format\n",
        "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "        # You can enable optimizations if needed. For example, default optimizations:\n",
        "        # converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "        # For more control over quantization (e.g., float16 or int8),\n",
        "        # you would add more configurations here. For example:\n",
        "        # converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "        # converter.target_spec.supported_types = [tf.float16] # For float16 quantization\n",
        "\n",
        "        tflite_model = converter.convert()\n",
        "        print(\"Model converted to TensorFlow Lite format successfully.\")\n",
        "\n",
        "        # Save the TFLite model to a file\n",
        "        with open(tflite_model_path, 'wb') as f:\n",
        "            f.write(tflite_model)\n",
        "        print(f\"TensorFlow Lite model saved to: {tflite_model_path}\")\n",
        "        print(f\"Size of TFLite model: {os.path.getsize(tflite_model_path) / (1024):.2f} KB\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during model loading or conversion: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}